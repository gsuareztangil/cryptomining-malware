
import os, sys, json, pickle
from datetime import datetime, timedelta
import pprint
import re
from scapy.all import rdpcap
import collections

try:
    from urllib.parse import urlparse
except ImportError:
     from urlparse import urlparse

from VirusTotal.crawl_reports import retrieve_intelligence_many, read_hashes_from_file
#from mergeWallets import get_first_seen


LIST_HASHES = [
  '../data/minersForumsSHA2_vetted.txt', 
  '../data/minersForumsMD5s_vetted_semi.txt', 
  '../data/ElfsHashList.txt', 
  '../data/minersVirusShare.csv', 
  '../data/minersVirusShare-317-325.csv',
  '../data/miners_VT_2017-11-20.csv', 
  '../data/miners_VT_2017-12-05.csv', 
  '../data/miners_VT_2018-03-14.csv', 
  '../data/miners_VT_2018-06-miners.csv',
  '../data/miners_reportedAV_VT_2017-11-20.csv', 
  '../data/miners_reportedAV_VT_2017-12-05.csv', 
  '../data/miners_reportedAV_VT_2018-03-14.csv',
  '../data/VirusTotal/2018-06-miners_directory-listing.txt',
  #'../data/minersPaloAltoNetworks_NaN.txt',
  '../data/minersPaloAltoNetworks.csv',
  '../data/parents-linkedwallets-withoutpool.txt', 
  '../data/miners_HybridAnalysis.txt'
]

# Note we don't share the raw data from VT. See VirusTotal/crawl_reports.py for the API client implementation
PATH_SEARCHES_FOLDER = '../data/VirusTotal/searches'
REPORTS_FOLDER = '../data/VirusTotal/reports'
BEHAVIORS_FOLDER = '../data/VirusTotal/behaviours'
NETWORKS_FOLDER = '../data/VirusTotal/network'
NETWORKS_HYBRID_FOLDER = '../data/VirusTotal/network-hybrid'
BINS_FOLDER = '../data/VirusTotal/bin'

DATA_SOURCES = {'report': REPORTS_FOLDER, 'behaviour': BEHAVIORS_FOLDER, 'pcap1': NETWORKS_FOLDER, 'pcap2': NETWORKS_HYBRID_FOLDER, 'bin': BINS_FOLDER}

DATASET_PATH = '../data/dataset.json'

def read_list_hashes_searches():

    files = []
    # ------- READ ALL JSONS FORM SEARCHES DIR
    for filename in os.listdir(PATH_SEARCHES_FOLDER):
        if not 'executables' in filename:
            continue # Skip JS for the time being
        if filename.endswith('json'):
            pathfile = os.path.join(PATH_SEARCHES_FOLDER, filename)
            files.append(pathfile)
    return files


def get_extension(resource):
    if 'pcap' in resource:
        return '.pcap'
    if 'bin' in resource:
        return ''
    return '.'+resource

def get_resources(sample):

    resources = []

    for resource in DATA_SOURCES:
        folder = DATA_SOURCES[resource]
        path = os.path.join(folder, sample) + get_extension(resource)
        #print '[i] get_resources: processing', path

        if os.path.isfile(path):

            if resource == 'report' or resource == 'behaviour':
                json = pickle.load(open(path, "rb" ))

                if resource == 'report' and 'sha256' in json and 'additional_info' in json: 
                    resources.append(resource)
                elif resource == 'behaviour':
                    resources.append(resource)

            elif 'pcap' in resource: 
                try:
                    packets=rdpcap(path)
                    resources.append(resource)
                except Exception as e:
                    pass
            elif resource == 'bin':
                if os.stat(path).st_size > 0:
                    resources.append(resource)
            else:
                print '[e] Unknown resource'
                raise

    return resources

def get_packers(report):
    MIN_ENTROPY=7.5
    packers=[]
    packerPresent=False
    compressed=False
    isAutoIt=False
    entropy=0
    if 'additional_info' in report and 'f-prot-unpacker' in report['additional_info']:
        packer=report['additional_info']['f-prot-unpacker'].replace(',',';').strip()
        packerList=packer.split(';')
        for p in packerList:
            pstrip=p.strip()
            if 'UTF-8' in pstrip or 'Unicode' in pstrip:
                continue
            elif 'AutoIt' in pstrip:
                isAutoIt=True
                continue
            elif 'zip' in pstrip.lower() or 'rar' in pstrip.lower() or '7z' in pstrip.lower():
                compressed=True
                continue
            packerPresent=True
            if not pstrip in packers:
                packers.append(pstrip)
            
        if 'additional_info' in report and 'exiftool' in report['additional_info'] and 'CompiledScript' in report['additional_info']['exiftool']:
            if 'autoit' in report['additional_info']['exiftool']['CompiledScript'].lower():
                isAutoIt=True

        if 'additional_info' in report and 'pe-overlay' in report['additional_info']:
            try:
                entropy=float(report['additional_info']['pe-overlay']['entropy'])
                if not packerPresent and not compressed and not isAutoIt and entropy>MIN_ENTROPY:
                    packers.append('OTHER')
                    packerPresent=True
                if isAutoIt and entropy>MIN_ENTROPY:
                    packers.append('AutoIt')
                    packerPresent=True
            except:
                pass
    if not packerPresent:
        if compressed:
            packers.append('COMPRESSED')
        else:
            packers.append('RAW')
    return list(set(packers))

def fetch_dataset_metainfo(all_origins):

    dataset = {}

    #Get the list of KNOWN-MINER in our data sources
    print '[i] Get the list of KNOWN-MINER in our data sources'
    for origen in all_origins:
        
        hash_samples = read_hashes_from_file(origen)
        
        for sample in hash_samples:
            
            #Initialize stuff and query the relevant metainfo for the sample
            if not sample in dataset:

                dataset[sample] = {}
                dataset[sample]['type'] = 'MINER' # Known miner
                dataset[sample]['origen'] = []
                dataset[sample]['fs'] = None

                dataset[sample]['resources'] = []
                resources = get_resources(sample)
                dataset[sample]['resources'].extend(resources)

            #Annotate the origin, i.e.: the source of the dataset in which the sample was found
            if origen not in dataset[sample]['origen']:
                dataset[sample]['origen'].append(origen)  


    #Get the list of RANCILLARY resources
    #This also populates the VT metatada of the other samples in the dataset
    print '[i] Get the list of RANCILLARY resources'
    for report in os.listdir(REPORTS_FOLDER):
        if report.endswith('report'):

            path = os.path.join(REPORTS_FOLDER, report)
            #print '[i] fetch_dataset_metainfo: processing', path
            json_report = pickle.load(open(path, "rb" ))

            if 'sha256' in json_report:
                sample = json_report['sha256']

                #Checking if both the sha256 and the md5 of the same sample are in the dataset 
                if json_report['md5'] in dataset:
                    print 'Fix md5 vs sha256 in VT files', json_report['md5'], sample
                    tmp = dataset.pop(json_report['md5']) 
                    if sample not in dataset: 
                        dataset[sample] = tmp
                    else:
                        for origen in tmp['origen']:
                            if origen not in dataset[sample]['origen']:
                                dataset[sample]['origen'].append(origen)
                        for resource in tmp['resources']:
                            if resource not in dataset[sample]['resources']:
                                dataset[sample]['resources'].append(resource)

                #For ANCILLARY
                if not sample in dataset:
                    dataset[sample] = {}
                    dataset[sample]['type'] = 'ANCILLARY' # Ancillary/auxiliary binary related to a miner
                    dataset[sample]['origen'] = ['VT_parents']
                    dataset[sample]['fs'] = None

                    dataset[sample]['resources'] = []
                    resources = get_resources(sample)
                    dataset[sample]['resources'].extend(resources)

                #Populate this info in all samples where there is a report (not only ancillary)
                dataset[sample]['ITW_urls'] = None
                if 'ITW_urls' in json_report:
                    dataset[sample]['ITW_urls'] = json_report['ITW_urls']
                dataset[sample]['packers'] = get_packers(json_report)
                if 'first_seen' in json_report:
                    dataset[sample]['fs'] = json_report['first_seen']

                dataset[sample]['positives'] = json_report['positives']
                if dataset[sample]['positives'] < 10 and dataset[sample]['type'] != 'ANCILLARY':
                    print '[e] Sample with few positives', sample, dataset[sample]['positives']
                    #raise Exception('Are we including begins?')


    with open(DATASET_PATH, 'w') as f:
        json.dump(dataset, f)

    return dataset


def get_date_object(vt_date):
    date_object = None
    if not vt_date:
        return None
    if '.' in vt_date and 'T' in vt_date and 'Z' in vt_date:
        date_object = datetime.strptime(vt_date, '%Y-%m-%dT%H:%M:%S.%fZ')
    elif vt_date:
        date_object = datetime.strptime(vt_date, '%Y-%m-%d %H:%M:%S')
    return date_object



linesMalware=open('../data/not_malware_samples.csv').readlines()
def is_not_malware(sample):
    for l in linesMalware:
        if 'REMOVE' in l and sample in l:
            return True
    return False    

def print_stats(dataset):

    num_miners = 0
    num_ancillary = 0

    num_pa = 0
    num_vt = 0
    num_vs = 0
    num_forums = 0
    num_ha = 0

    num_reports = 0
    num_behaviours = 0
    num_pcaps = 0
    num_pcaps = 0
    num_bin = 0

    date_first = None
    date_last = None

    for sample in dataset:

        if is_not_malware(sample): 
            continue

        if dataset[sample]['type'] == 'MINER': 
            num_miners += 1
        else: 
            num_ancillary += 1

        # --- Measure the source of the dataset
        for origen in set(dataset[sample]['origen']): 
            if 'PaloAltoNetworks' in origen:
                num_pa += 1
            elif 'VT' in origen or 'searches' in origen: 
                num_vt += 1
            elif 'VirusShare' in origen:
                num_vs += 1
            elif 'Forums' in origen:
                num_forums += 1
            elif 'HybridAnalysis' in origen: 
                num_ha += 1

        # --- Measure the resources of the dataset
        if 'report' in dataset[sample]['resources']:
            num_reports += 1
        if 'behaviour' in dataset[sample]['resources']:
            num_behaviours += 1
        if 'pcap1' in dataset[sample]['resources']:
            num_pcaps += 1
        if 'pcap2' in dataset[sample]['resources']:
            num_pcaps += 1
        if 'bin' in dataset[sample]['resources']:
            num_bin += 1

        date_object = get_date_object(dataset[sample]['fs'])

        if not date_object:
            continue

        if not date_first:
            date_first = date_object
        if not date_last:
            date_last = date_object

        if date_object < date_first:
            date_first = date_object

        if date_object > date_last:
            date_last = date_object


    print '\\begin{table}'
    print '\\centering'
    print '\\begin{tabular}{c|l|r}'
    print '\\hline'
    print '\\bf Category  & \\bf Type & \\bf \\#Samples \\\\'
    print '\\hline'

    print '\\multirow{3}{*}{Summary} &\t', 'ALL EXECUTABLES & \t', "{:,}".format(len(dataset)), '\\\\'
    print ' & \t Miner Binaries & \t', "{:,}".format(num_miners) , '\\\\'
    print ' & \t Ancillary Binaries & \t', "{:,}".format(num_ancillary) , '\\\\'
    #print '----- SOURCES -----'
    print '\\hline'
    print '\\multirow{4}{*}{Sources} &\t', 'Palo Alto Networks & \t', "{:,}".format(num_pa) , '\\\\'
    print ' & \t Virus Total & \t', "{:,}".format(num_vt) , '\\\\'
    print ' & \t Virus Share & \t', "{:,}".format(num_vs) , '\\\\'
    print ' & \t Hybrid Analysis & \t', "{:,}".format(num_ha) , '\\\\'
    print '% & \t Forums & \t', "{:,}".format(num_forums) , '\\\\'
    #print '----- RESOURCES -----'
    print '\\hline'
    print '\\multirow{3}{*}{Resources} &\t', 'Sandbox Analysis & \t', "{:,}".format(num_reports) , '\\\\' #+ num_behaviours 
    print ' & \t Network Analysis & \t', "{:,}".format(num_pcaps) , '\\\\'
    print ' & \t Binary Analysis & \t', "{:,}".format(num_bin) , '\\\\'

    print '\\hline'
    print '\\end{tabular}'
    print '\\caption{Summary of our dataset. Data collection ranges from ' + str(date_first) + ' to ' + str(date_last) + '.}\\label{tab:dataset}'
    print '\\end{table}'


def look_at_itw_url(dataset):
    aggregateIf={
    'github.com':['github'],
    '*.google.com':['google'],
    '*.amazonaws.com':['amazonaws.com'],
    'dropbox.com':['dropboxusercontent'],
    '*.4sync.com':['4sync'],
    'coinhive.com':['coinhive','coin-hive'],
    'minergate.com':['minergate']
    #'beget.tech':['beget.tech']
    }
    itw_netloc = {}

    total_samples = {}
    total_urls = {}
    patternIP = re.compile("[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}")
    #sample_list=dataset
    campaigns=pickle.load(open('../data/aggregated_campaigns_second_round.pickle'))
    sample_list=[]
    for c in campaigns:
        sample_list.extend(campaigns[c]['samples'])
    sample_list=list(set(sample_list))

    for sample in sample_list:
        if not sample in dataset or not 'ITW_urls' in dataset[sample] or dataset[sample]['ITW_urls'] is None or len(dataset[sample]['ITW_urls']) <= 0:
            continue
        for url in dataset[sample]['ITW_urls']:
            #print sample, url
        
            parsed_uri = urlparse(url)
            #parsed = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)            
            parsed = '{uri.netloc}'.format(uri=parsed_uri)
            parsed=parsed.split(':')[0] 
            if patternIP.match(parsed):
                continue
            for name in aggregateIf:
                for value in  aggregateIf[name]:
                    if value in parsed:
                        parsed=name       
            if parsed == 'coinhive.com':
                print 'legacy?', sample, parsed, dataset[sample]['fs'].split()[0].split('-')[:-1]
            if parsed not in itw_netloc: 
                itw_netloc[parsed] = {'urls': [], 'samples': []}

            if url not in itw_netloc[parsed]['urls']:
                itw_netloc[parsed]['urls'].append(url)

            if sample not in itw_netloc[parsed]['samples']:
                itw_netloc[parsed]['samples'].append(sample)

            if sample not in total_samples:
                total_samples[sample] = True

            if url not in total_samples:
                total_urls[url] = True


    print '\\begin{table}'
    print '\\centering'
    print '\\begin{tabular}{l|r|r}'
    print '\\hline'
    print '\\bf Domains  & \\bf \\#Samples & \\bf \\#URLs \\\\'
    print '\\hline'
    for idx, item in enumerate(sorted(itw_netloc.items(), key=lambda t: len(t[1]['samples']), reverse=True)):
        key = item[0]
        to_print = str(key) + ' &\t ' + str(len(itw_netloc[key]['samples'])) + ' &\t ' + str(len(itw_netloc[key]['urls'])) + '\\\\'
        # if idx > 9: 
        #     if abs(len(itw_netloc[key]['samples']) - len(itw_netloc[key]['urls'])) < 4 or len(itw_netloc[key]['samples']) < 5:
        #         if 'goo.gl' != key:
        #             to_print = '%' + to_print  
        #             #continue

        print to_print

    print '\\hline'
    print 'TOTAL \#: ' + str(len(itw_netloc)) + '(\# domains) &\t ' + str(len(total_samples)) + ' &\t ' + str(len(total_urls))  + '\\\\'
    print '\\hline'
    print '\\end{tabular}'
    print '\\caption{ Complete list of domains hosting known mining malware, number of samples hosted under each domain and number of URLs hosting those samples.}\\label{tab:itw_urls_all}'
    print '\\end{table}'

    print '\\begin{table}'
    print '\\centering'
    print '\\begin{tabular}{l|r|r}'
    print '\\hline'
    print '\\bf Domains  & \\bf \\#Samples & \\bf \\#URLs \\\\'
    print '\\hline'
    N=10
    for idx, item in enumerate(sorted(itw_netloc.items(), key=lambda t: len(t[1]['samples']), reverse=True)[:N]):
        key = item[0]
        to_print = str(key) + ' &\t ' + str(len(itw_netloc[key]['samples'])) + ' &\t ' + str(len(itw_netloc[key]['urls'])) + '\\\\'
        # if idx > 9: 
        #     if abs(len(itw_netloc[key]['samples']) - len(itw_netloc[key]['urls'])) < 4 or len(itw_netloc[key]['samples']) < 5:
        #         if 'goo.gl' != key:
        #             to_print = '%' + to_print  
        #             #continue

        print to_print

    print '\\hline'
    print 'TOTAL \#: ' + str(len(itw_netloc)) + '(\# domains) &\t ' + str(len(total_samples)) + ' &\t ' + str(len(total_urls))  + '\\\\'
    print '\\hline'
    print '\\end{tabular}'
    print '\\caption{ Excerpt of domains hosting known mining malware, number of samples hosted under each domain and number of URLs hosting those samples.}\\label{tab:itw_urls_excerpt}'
    print '\\end{table}'




def build_dataset():
    files = read_list_hashes_searches()
    LIST_HASHES.extend(files)
    dataset = fetch_dataset_metainfo(LIST_HASHES)


def debug():

    dataset = json.load(open(DATASET_PATH))
    print '---- Checking out the ITW_urls'
    look_at_itw_url(dataset)

    sys.exit()

if __name__ == "__main__" :
    
    #debug()

    # ----- load all resources:
    build_dataset()
    dataset = json.load(open(DATASET_PATH))
    look_at_itw_url(dataset)
    # ----- print overview dataset:
    print_stats(dataset)


