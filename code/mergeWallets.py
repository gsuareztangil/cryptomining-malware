#import virustotal
import sys, os, time, json, pickle, csv, shutil
import copy
import requests
from random import randint
import hashlib
import pprint
import pandas as pd
import collections, operator
import math
from datetime import datetime, timedelta
import locale

import networkx as nx

from extractData import knownPools
from domainAnalyser import analyse_campaign_by_wallets,get_activity_period
from walletAnalyser import is_donation_wallet,is_not_malware,dataset,stats_wallets,get_currency,exchange_addresses,XMR_USD_EXCHANGE
import search4childs


PATH_REPORTS =  '../data/VirusTotal/reports'
FONTSIZE = 40
MONERO_EXCHANGE = XMR_USD_EXCHANGE

locale.setlocale(locale.LC_ALL,"")

def get_wallets_per_samples(filename='../data/data_from_samples_miners.pickle', withpool=False):
    wallets = {}
    samples=pickle.load(open(filename))
    for sha256 in samples:
        if not 'user' in samples[sha256]:
            continue
        if not samples[sha256]['user'] or samples[sha256]['user'].lower() == 'nan':
            continue
        try:
            user=samples[sha256]['user'].encode('utf-8').replace('"','\'')
        except UnicodeDecodeError:
            continue
        for wallet in user.split(','):
            if len(wallet) <= 3:
                continue
            wallet = wallet.decode('ascii', 'replace').strip()
            if not wallet or wallet.lower() == 'nan':
                continue
            if wallet == 'USES-CONFIG-FILE' or is_donation_wallet(wallet):
                continue
            if len(wallet) <= 3:
                continue
            if len(wallet)>90 and wallet[0]=='4':
                if wallet.split('.')[0] in exchange_addresses and len(wallet.split('.'))>1:
                    wallet=".".join(wallet.split('.')[:2]).split('+')[0].split('/')[0].split(":")[0].split('@')[0] 
                else:
                    wallet=wallet.split('.')[0].split('+')[0].split('/')[0].split(":")[0].split('@')[0]

            if withpool:
                if 'pool' in samples[sha256] and samples[sha256]['pool'].lower() != 'nan':
                    try:
                        pool=samples[sha256]['pool'].encode('utf-8').replace('"','\'')
                    except UnicodeDecodeError:
                        pool = ''
                    pool = pool.decode('ascii', 'replace').strip()
                else:
                    pool = ''
                address = unicode(wallet) + u'^' + unicode(pool)
            else:
                address = unicode(wallet)


            if sha256 not in wallets:
                wallets[sha256] = []

            if address not in wallets[sha256]:
                wallets[sha256].append(address)

    return wallets

def aggregate_samples(report_folder):

    compressed_parents_map = {}
    execution_parents_map = {}
    ssdeep_map = {} 
    authentihash_map = {}
    authentihash = {}

    for report in os.listdir(report_folder):
        if report.endswith('report'):
            json_report = pickle.load( open( os.path.join(report_folder, report), "rb" ))
            #print pprint.pprint(json_report)
            if 'sha256' in json_report:
                #print json_report['sha256'],'\t', json_report['positives'], json_report['malicious_votes'], '\t', json_report['first_seen']
                
                # ------ MERGE BY compressed_parents
                if 'additional_info' in json_report and 'compressed_parents' in json_report['additional_info']:
                    if json_report['sha256'] not in compressed_parents_map:
                        compressed_parents_map[json_report['sha256']] = []
                    compressed_parents_map[json_report['sha256']].extend(json_report['additional_info']['compressed_parents'])

                # ------ MERGE BY compressed_parents
                if 'additional_info' in json_report and 'execution_parents' in json_report['additional_info']:
                    if json_report['sha256'] not in execution_parents_map:
                        execution_parents_map[json_report['sha256']] = []
                    execution_parents_map[json_report['sha256']].extend(json_report['additional_info']['execution_parents'])

                # ------ MERGE BY ssdeep
                if 'ssdeep' in json_report and json_report['ssdeep']:                  
                    ssdeep_map[json_report['sha256']] = json_report['ssdeep']

                # ------ MERGE BY authentihash
                if 'authentihash' in json_report and json_report['authentihash']:
                    authentihash_map[json_report['sha256']] = json_report['authentihash']

                    if json_report['sha256'] not in authentihash:
                        authentihash[json_report['authentihash']] = []                    
                    authentihash[json_report['authentihash']].append(json_report['sha256'])

            else: 
                print report, 'was not available'

        #sys.exit()

    #print pprint.pprint(compressed_parents_map)

    return compressed_parents_map, ssdeep_map, authentihash_map, authentihash, execution_parents_map


def plot_campaigns(wallets, compressed_parents, execution_parents, childs, campaigns, mining_tools, add_compressed_parents=False,linkedwallets=False, withpool=False, duplicate_childs=True,remove_botnets=False):
    
    G = nx.Graph()
    seen = []
    seen_with_wallets = []
    # ------ Add samples with wallets 
    for sample in wallets:
        if is_not_malware(sample):
            continue
        if sample in dataset and dataset[sample]['fs'] is not None:
            vt_date=dataset[sample]['fs']
            month,quarter,semester=get_vt_period(vt_date)
        else:
            month='UNKNOWN'
            quarter='UNKNOWN'
            semester='UNKNOWN'
        wallet_sample = ','.join(wallets[sample]).strip()

        if sample not in seen:
            if wallet_sample and len(wallet_sample) > 0:
                #AD-HOC REMOVING INVALID CHARACTER ENCODING
                if sample=='b0ac31cb7fcbbbec0c26cb3bb549b0b5c6e7e9119f72828ea18652a06ddcdbae':
                    wallet_sample='\\\\x120H00H0'
                if sample in mining_tools:
                    print '[e] Donation wallet not filtered? Details:', sample, wallet_sample
                    #raise Exception('Donation wallet not filtered?')
                    continue
                seen.append(sample)    
                seen_with_wallets.append(sample)

                positives = -1
                if sample in dataset and 'positives' in dataset[sample]:
                    positives = dataset[sample]['positives']

                G.add_node(sample, label='',monthTS=month,quarterTS=quarter,semesterTS=semester, wallet=wallet_sample, positives=positives, color='#CAFFC5', type='wallet_node', fillcolor='#CAFFC5', style='filled')

    # ----- Link samples with the same wallets
    if linkedwallets:
        reverse_wallets = {}
        for sample in wallets:
            if is_not_malware(sample):
                continue  
            if sample in mining_tools:
                continue # Donation wallet not filtered         
            for wallet in wallets[sample]:
                if wallet not in reverse_wallets:
                    reverse_wallets[wallet] = []
                if sample not in reverse_wallets[wallet]:
                    reverse_wallets[wallet].append(sample)

        for wallet in reverse_wallets: 
            if len(reverse_wallets[wallet]) < 1:
                continue
            linkernode = "'linkedwallets@%s'"%wallet.replace(':','_').replace('\\U','\\_U')
            if len(wallet)<7:
                label=wallet
            else:
                label=wallet[:5]+'...'
            G.add_node(linkernode, label=label,wallet=wallet.replace(':','_'), color='#3820FF', type='linkedwallets', fillcolor='#3820FF', style='filled')
            for sample in reverse_wallets[wallet]:
                G.add_edge(sample, linkernode, weight=1, color='black', typeEdge='linkedwallets')
   
    # ------ Add parents of samples with wallets 
    #for sample in seen_with_wallets:
    # ------ Add parents of seen samples
    for sample in seen:

        if sample in dataset and dataset[sample]['fs'] is not None:
            vt_date=dataset[sample]['fs']
            month,quarter,semester=get_vt_period(vt_date)
        else:
            month='UNKNOWN'
            quarter='UNKNOWN'
            semester='UNKNOWN'  
        if add_compressed_parents:
            if sample in compressed_parents:

                parents = compressed_parents[sample]
                num_parents = float(len(parents))

                for parent in parents: 
                    positives = -1
                    if parent in dataset and 'positives' in dataset[parent]:
                        # Do not add potential compressed parents that are recopilations of various malware samples
                        positives = dataset[parent]['positives']
                        if positives<10:
                            continue
                    if parent not in seen:
                        G.add_node(parent, label='',monthTS=month,quarterTS=quarter,semesterTS=semester, wallet='', positives=positives, color='#FF0910', type='cparent_none', fillcolor='#FF0910', style='filled')
                        seen.append(parent)

                    G.add_edge(parent, sample, weight=1, color='black', typeEdge='same_family')


        if sample in execution_parents:

            parents = execution_parents[sample]
            num_parents = float(len(parents))

            for parent in parents: 

                if parent not in seen:
                    positives = -1
                    if parent in dataset and 'positives' in dataset[parent]:
                        positives = dataset[parent]['positives']

                    G.add_node(parent,label='', monthTS=month,quarterTS=quarter,semesterTS=semester, wallet='', positives=positives, color='#FF0910', type='eparent_none', fillcolor='#FF0910', style='filled')
                    seen.append(parent)

                G.add_edge(parent, sample, weight=1, color='black', typeEdge='same_family')

    #--------- Aggregate by domain and proxy
    for subcampaign in campaigns:
        if subcampaign.startswith('PROXY'):
            name='PROXY'
            label=subcampaign
            color = '#A3A3A3'
            nodetype ='domain_subcampaign_proxy'
        elif subcampaign.startswith('PRIVATE_POOL'):
            name='PRIVATE-POOL'
            label=subcampaign
            color = '#B7B7B7'
            nodetype ='domain_subcampaign_privatePool'
        elif subcampaign.startswith('ALIAS'):
            name="ALIAS_"+"-".join(subcampaign.split('-')[1:])
            label="-".join(subcampaign.split('-')[1:])
            color = '#CCCCCC'
            nodetype ='domain_subcampaign_alias'
        elif subcampaign.startswith('ITWURL'):
            name="ITWURL_"+"-".join(subcampaign.split('-')[1:])
            label="-".join(subcampaign.split('-')[1:])
            color = '#FFB6DD'
            nodetype ='domain_subcampaign_itwurl'            
        else:
            if remove_botnets:
                toRemove=["RAMNIT","VIRUT","NITOL-BOTNET"]
                add=True
                for botnet in toRemove:
                    if botnet in subcampaign:
                        add=False
                if not add:
                    continue
            name="OSINT_"+"-".join(subcampaign.split('-')[1:])
            color = '#F4F4F4'
            label="-".join(subcampaign.split('-')[1:])
            nodetype ='domain_subcampaign_osint'
        subcampaign_name = "'sub-%s'"%subcampaign.replace(':','_')
        
        addedSubcampaign=False
        for sample in campaigns[subcampaign]['samples']:
            if is_not_malware(sample):
                continue            
            if sample in dataset and dataset[sample]['fs'] is not None:
                vt_date=dataset[sample]['fs']
                month,quarter,semester=get_vt_period(vt_date)
            else:
                month='UNKNOWN'
                quarter='UNKNOWN'
                semester='UNKNOWN'  
            addEdge = False          
            # Do not include samples for which we have ITWURL, since these include (benign) mining tools, e.g. minexmr.exe
            # The consequence is that only ITW_URLs having samples with wallets will be displayed, and zero-wallet campaigns might appear
            # Also, some of the OSINT campaigns include benign mining tools, so these are checked against the whitelist
            # if sample not in seen and not subcampaign.startswith('ITWURL') and not sample in mining_tools:
            #     positives = -1
            #     if sample in dataset and 'positives' in dataset[sample]:
            #         positives = dataset[sample]['positives']

            #     G.add_node(sample, monthTS=month,quarterTS=quarter,semesterTS=semester, wallet='', positives=positives, color='subcampaign_sample', fillcolor='subcampaign_sample', style='filled')
            #     seen.append(sample)
            #     addEdge=True
            # elif sample in seen:
            #     addEdge=True
            if sample in seen:
                if not addedSubcampaign:
                    G.add_node(subcampaign_name,label=label, wallet='', campaign='sub-'+name, color=color, fillcolor=color, type=nodetype, style='filled', fontsize=FONTSIZE)
                    addedSubcampaign=True
                G.add_edge(sample, subcampaign_name, weight=1, color='black', typeEdge='same_subcampaign')    

    # -------- Add childs of samples with wallets
    #for sample in seen_with_wallets:
    # -------- Add childs of seen samples 
    # for sample in seen:

    #     if sample not in childs:
    #         continue

    #     for idx, hash_child in enumerate(childs[sample]):
            
    #         child = hash_child

    #         if child in dataset and dataset[child]['fs'] is not None:
    #             vt_date=dataset[child]['fs']
    #             month,quarter,semester=get_vt_period(vt_date)
    #         else:
    #             month='UNKNOWN'
    #             quarter='UNKNOWN'
    #             semester='UNKNOWN'  
    #         # Childs are tricky as they could be goodlware and/or not developed by this actor
    #         # Thus, in this mode bellow, we account for the childs, but we asign a unique node ID 
    #         if duplicate_childs:
    #             child = child + '-parent:' + sample + '_i:' + str(idx) 

    #         # Add details about the mining tool
    #         color = 'child_node'
    #         tool='no-tool'
    #         if hash_child in mining_tools:
    #             color = 'child_node_mining_tool'
    #             child = child + '-mining_tools:' + mining_tools[hash_child]['tool']
    #             tool=mining_tools[hash_child]['tool']
            

    #         if child not in seen:

    #             positives = -1
    #             if hash_child in dataset and 'positives' in dataset[hash_child]:
    #                 positives = dataset[hash_child]['positives']

    #             G.add_node(child, monthTS=month,quarterTS=quarter,semesterTS=semester, wallet='', positives=positives, color=color, fillcolor=color, style='filled',tool=tool)
    #             seen.append(child)

    #         G.add_edge(sample, child, weight=1, color='same_family')


    # outfile =  '../data/graph_campaigns.gexf'

    # if linkedwallets:
    #     outfile = outfile.replace('.gexf', '-linkedwallets.gexf')

    # if not withpool:
    #     outfile = outfile.replace('.gexf', '-withoutpool.gexf')

    # if duplicate_childs:
    #     outfile = outfile.replace('.gexf', '-duplicate_childs.gexf')


    # print '----- Dumping', outfile
    # print len(G.edges())
    # print len(G.nodes())
    # nx.write_gexf(G, outfile)  

    return G



def split_campaigns_and_evaluate_connected_subgraphs(inputfile = '../data/graph_wallets_all.gexf',aggregated_file_name='../data/aggregated_campaigns.pickle',writeToFile=False):
    print "Reading graph from %s"%inputfile
    G = nx.read_gexf(inputfile)
    print "Calculating connected components"
    connected_components = list(nx.connected_component_subgraphs(G))

    # Split campaigns
    print "Split campaigns"
    dirname = inputfile.replace('.gexf', '')
    if os.path.exists(dirname):
        print 'Deleting contents in', dirname
        shutil.rmtree(dirname)
    os.mkdir(dirname)

    for i, graph in enumerate(connected_components): 
        filename = 'C' + str(i) + '.gexf'
        outfile = os.path.join(dirname, filename)
        try:
            nx.write_gexf(graph, outfile) 
            print 'Dumped at', outfile
        except:
            print 'Cannot dump gexf. Dumping pickle...', outfile
            pickle.dump(graph,open(outfile.replace('gexf','pickle'),'wb'))

        

    # Evaluate_connected_subgraphs
    return evaluate_connected_subgraphs(G, aggregated_file_name=aggregated_file_name,writeToFile=True, connected_components=connected_components) 


def evaluate_connected_subgraphs(G,aggregated_file_name=None,writeToFile=False, connected_components=None):

    if not connected_components:
        connected_components = list(nx.connected_component_subgraphs(G))

    campaigns = {}
    for i, graph in enumerate(connected_components):
        
        campaigns[i] = {'wallets': [], 'samples': []}

        for edge in graph.nodes(data=True):
            #print edge
            sample = edge[0]

            # --- We do not account for artificial nodes added to link wallets or campaigns
            if 'linkedwallets' in sample:
                continue
            if 'sub-' in sample:
                continue
            if 'ITWURL' in sample:
                continue
            if '-' in sample: 
                print '[w] Removing tags from child/miner-tool', sample
                sample = sample.split('-')[0]

            try:
                wallets = filter(None, list(set(edge[1].get('wallet', None).split(','))))
            except AttributeError:
                print 'Error while processint edge in campaign', i, edge
                raise

            if sample not in campaigns[i]['samples']:
                campaigns[i]['samples'].append(sample)

            campaigns[i]['wallets'].extend(wallets)
            campaigns[i]['wallets'] = list(set(campaigns[i]['wallets']))

    if writeToFile:
        # write output to csv file
        with open(aggregated_file_name.replace('.pickle','.csv'), 'wb') as f:
            w = csv.DictWriter(f, fieldnames=['campaign', 'samples', 'wallets'])
            w.writeheader()

            for row in campaigns.items():
                w.writerow({'campaign': row[0], 'samples': ','.join(row[1]['samples']), 'wallets': ','.join(row[1]['wallets'])})


        with open(aggregated_file_name,'wb') as f:
            pickle.dump(campaigns, f)

    return campaigns 


def evaluate_subgraphs_from_directory(directory_name='../data/graph_wallets_all_second_round',aggregated_file_name='../data/aggregated_campaigns_second_round.pickle', writeToFile=False):

    campaigns = pickle.load(open('../data/aggregated_campaigns_all.pickle')) 

    for campaign in campaigns:
        seen=[]
        try:
            graph = nx.read_gexf(directory_name+'/C%s_updated.gexf'%campaign)
        except:
            print "[e] Could not read graph: C%s_updated.gexf"%campaign
            continue

        for edge in graph.nodes(data=True):
            #print edge
            sample = edge[0]

            # --- We do not account for artificial nodes added to link wallets or campaigns
            if 'linkedwallets' in sample:
                continue
            if 'sub-' in sample:
                continue
            if 'ITWURL' in sample:
                continue
            if '-' in sample: 
                print '[w] Removing tags from child/miner-tool', sample
                sample = sample.split('-')[0]

            if sample not in campaigns[campaign]['samples']:
                campaigns[campaign]['samples'].append(sample)

    if writeToFile:
        # write output to csv file
        with open('../data/campaigns_second_round.csv', 'wb') as f:
            w = csv.DictWriter(f, fieldnames=['campaign', 'samples', 'wallets'])
            w.writeheader()

            for row in campaigns.items():
                w.writerow({'campaign': row[0], 'samples': ','.join(row[1]['samples']), 'wallets': ','.join(row[1]['wallets'])})

        with open(aggregated_file_name,'wb') as f:
            pickle.dump(campaigns, f)

    return campaigns 

def plot_subgraphs_from_directory(directory_name='../data/graph_wallets_second_round',aggregated_file_name='../data/aggregated_campaigns_all.pickle'):

    from networkx.drawing.nx_pydot import write_dot

    campaigns = pickle.load(open(aggregated_file_name)) 

    for campaign in campaigns:

        seen=[]
        try:
            graph = nx.read_gexf(directory_name+'/C%s_updated.gexf'%campaign)
        except:
            print "Graph of campaign %s cannot be loaded"%campaign
            continue
        outdir_dot = directory_name+'_dot'
        if not os.path.isdir(outdir_dot):
            os.mkdir(outdir_dot)
        file_dot = outdir_dot+'/C%s_updated.dot'%campaign
        if not os.path.exists(file_dot):
            try:
                write_dot(graph, file_dot)
            except:
                print "Dot of campaign %s cannot be written"%campaign
                continue 

            outdir_graph = directory_name+'_pdf'
            if not os.path.isdir(outdir_graph):
                os.mkdir(outdir_graph)
            file_pdf = outdir_graph+'/C%s_updated.pdf'%campaign

            command = "sfdp -x -Goverlap=scale -Tpdf %s > %s"%(file_dot, file_pdf)
            os.system(command)



def getPoolName(domain):
    for p in knownPools:
        if p in domain:
            return p


def get_value_totalpaid(campaigns, withpool=False,include_activity_period=False):

    # TODO call to: totalPaidUser,numSamples,poolData=stats_wallet(w,numTab=numTab,verbose=verbose,snapshot=snapshot)

    # INFO_FILE='../data/infoWallets/info_wallets.csv'
    # info_wallets = pd.read_csv(INFO_FILE,keep_default_na=False,na_values=['nan','nodata','NaN','N/A'])
    # print 'INFO: reading infoWallets from', INFO_FILE

    info_totalpaid = {}
    info_totalpaid_dic = {}
    value_totalpaid = {}
    still_active={}
    totalCampaignsWithData=0
    all_samples = 0
    all_wallets = 0
    all_paid = 0.0
    veryInitialDate=datetime.now()
    accounted_for = {}
    all_totalUSD=0

    for campaign_id in campaigns:
        wallets = campaigns[campaign_id]['wallets']
        total_paid = 0.0
        still_active[campaign_id]=False    
        totalPaid,totalSamples,totalWallets,poolData,totalUSD=stats_wallets(wallets,numTab=0,verbose=False)    
        firstPayment,lastActivity=get_activity_period(poolData)
        if lastActivity is None:
            continue
        if firstPayment is None:
            firstPayment=lastActivity

        now = datetime.now()
        # Active if lastActivity is within 1 week
        then = now - timedelta(days=7)
        still_active[campaign_id]=lastActivity >= then
        # for address in wallets:

        #     #print '----', address 

        #     pool = '*'
        #     if withpool: 
        #         split = address.split('^')
        #         pool = None
        #         wallet = None
        #         if len(split) > 0:
        #             pool = split[0]
        #             wallet = ''.join(split[1:])
        #             if pool and str(pool) == 'nan': 
        #                 print 'WARNING pool is NaN... skipping', address
        #                 pool = None
        #             else:
        #                 pool = getPoolName(pool)
        #                 rows = info_wallets.loc[(info_wallets['USER'] == wallet) & (info_wallets['POOL'] == pool)].TOTAL_PAID
        #     else:
        #         wallet = address
        #         rows = info_wallets.loc[(info_wallets['TOTAL_PAID'].notnull()) & (info_wallets['USER'] == wallet)].TOTAL_PAID
        #     if not wallet:
        #         print 'WARNING empty wallet... skipping', wallet, 'vs', address
        #         #No need to call to continue in here because next if will not pass and we want to keep track of the general stats

        #     if pool and wallet: #Specific pool or all (a.k.a. *)
        #         address_with_poolname = pool + '^' + wallet
        #         if not address_with_poolname in accounted_for: 
        #             #if rows.sum() != 0:
        #             #    print '----', campaign_id, pool, wallet, rows.sum()
        #             total_paid += rows.sum()
        #             accounted_for[address_with_poolname] = True
        if totalPaid is not None:
            total_paid=round(totalPaid)    
            totalCampaignsWithData+=1
        else:
            continue
        value_totalpaid[campaign_id] = total_paid
        if include_activity_period:
            if still_active[campaign_id]:
                info_totalpaid[campaign_id] = 'C\\#' + str(campaign_id) + ' & ' + str(len(campaigns[campaign_id]['samples']))  + ' & ' + str(len(campaigns[campaign_id]['wallets']))  + ' & ' + firstPayment.strftime("%m/%y") + ' to [active]* & ' +  "{0:,.0f}".format(total_paid) + ' & '+ millify(totalUSD) + '\\\\'
            else:
                info_totalpaid[campaign_id] = 'C\\#' + str(campaign_id) + ' & ' + str(len(campaigns[campaign_id]['samples']))  + ' & ' + str(len(campaigns[campaign_id]['wallets']))  + ' & ' + firstPayment.strftime("%m/%y") + ' to ' + lastActivity.strftime("%m/%y")+' & ' +  "{0:,.0f}".format(total_paid) + ' & '+ millify(totalUSD) + '\\\\'
            info_totalpaid_dic[campaign_id] = {'samples': len(campaigns[campaign_id]['samples']), 'wallets': len(campaigns[campaign_id]['wallets']), 'total_paid': total_paid, 'totalUSD': totalUSD, 'firstPayment':firstPayment,'lastActivity':lastActivity}
        else:
            info_totalpaid[campaign_id] = 'C\\#' + str(campaign_id) + ' & ' + str(len(campaigns[campaign_id]['samples']))  + ' & ' + str(len(campaigns[campaign_id]['wallets']))  + ' & ' +  locale.format("%.2f", total_paid, 2) + ' & '+ millify(totalUSD) + '\\\\'
            info_totalpaid_dic[campaign_id] = {'samples': len(campaigns[campaign_id]['samples']), 'wallets': len(campaigns[campaign_id]['wallets']), 'total_paid': total_paid, 'totalUSD': totalUSD}
        all_samples += len(campaigns[campaign_id]['samples']) 
        all_wallets += len(campaigns[campaign_id]['wallets']) 
        if firstPayment>datetime(2014,1,1) and firstPayment<veryInitialDate:
            veryInitialDate=firstPayment
        all_paid += total_paid
        all_totalUSD+=totalUSD

    return value_totalpaid, info_totalpaid_dic, info_totalpaid, all_paid, all_totalUSD,all_wallets, all_samples,still_active,totalCampaignsWithData,veryInitialDate


millnames = ['',' K',' M',' B',' Trillion']

def millify(n):
    n = float(n)
    millidx = max(0,min(len(millnames)-1,
                        int(math.floor(0 if n == 0 else math.log10(abs(n))/3))))

    return '{:.0f}{}'.format(n / 10**(3 * millidx), millnames[millidx])

def campaign2latex(campaigns, withpool=False,include_activity_period=False):

    value_totalpaid, info_totalpaid_dic, info_totalpaid, all_paid, all_totalUSD,all_wallets, all_samples,still_active,totalCampaignsWithData,veryInitialDate  = get_value_totalpaid(campaigns, withpool,include_activity_period=include_activity_period)
    #print '---- histogram of aggregated payments:'
    #print collections.Counter(value_totalpaid.values())

    print '---- Aggregated payments:'

    top_campaigns = []

    top_samples = 0
    top_wallets = 0
    top_paid = 0.0
    top_paid_usd = 0.0

    print '\\begin{table}'
    print '\\centering'
    if include_activity_period:
        print '\\begin{tabular}{lrrcrr}'
    else:
        print '\\begin{tabular}{lrrrr}'
    print '\\hline'
    if include_activity_period:
        print '\\bf Campaign & \\hspace{-1cm} \\bf \\#S & \\bf \\#W  & \\bf Period & \\bf XMR & \\bf \$ \\\\'
    else:
        print '\\bf Campaign & \\hspace{-1cm} \\bf \\#Samples & \\bf \\#Wallets  & \\bf Paid (XMR) & \\bf Earned \\\\'
    print '\\hline'
    #ordered=sorted(info_totalpaid_dic, key=lambda k:info_totalpaid_dic[k]['wallets'], reverse=True)
    ordered=sorted(info_totalpaid_dic, key=lambda k:info_totalpaid_dic[k]['samples'], reverse=True)
    topFirstPayment=datetime.now()
    for key, value in sorted(value_totalpaid.items(), key=operator.itemgetter(1), reverse=True)[:10]:
        value=value_totalpaid[key]
    # for key in ordered[:10]:
    #     value= info_totalpaid_dic[key]['samples']
        top_samples += info_totalpaid_dic[key]['samples']
        top_wallets += info_totalpaid_dic[key]['wallets']
        top_paid += info_totalpaid_dic[key]['total_paid'] 
        top_paid_usd += info_totalpaid_dic[key]['totalUSD'] 
        if info_totalpaid_dic[key]['firstPayment']<topFirstPayment:
            topFirstPayment=info_totalpaid_dic[key]['firstPayment']
        print info_totalpaid[key] 
        top_campaigns.append((key, value))
    print '\\hline'
    if include_activity_period:
        print 'TOP-10' +                       ' & ' + millify(top_samples) + ' & ' + str(top_wallets) + ' & '+topFirstPayment.strftime("%Y/%m/%d")+' - * &' + "{0:,.0f}".format(top_paid) + ' & ' +millify(top_paid_usd) + '\\\\'
        print 'ALL-' + str(totalCampaignsWithData) + ' & ' + millify(all_samples) + ' & ' + str(all_wallets) + ' & '+veryInitialDate.strftime("%Y/%m/%d")+' - * &' + "{0:,.0f}".format(all_paid) + ' & ' + millify(all_totalUSD) + '\\\\'
    else:
        print 'TOP-10' +                       ' & ' + str(top_samples) + ' & ' + str(top_wallets) + ' & ' + locale.format("%.2f", top_paid, 2) + ' & \$' + locale.format("%.2f", top_paid_usd, 2) + '\\\\'
        print 'ALL-' + str(totalCampaignsWithData) + ' & ' + str(all_samples) + ' & ' + str(all_wallets) + ' & ' + locale.format("%.2f", all_paid, 2) + ' & \$' + locale.format("%.2f", all_totalUSD, 2) + '\\\\'
    print '\\hline'
    print '\\end{tabular}'
    print '\\caption{Aggregated payments and statistics for the top most profitable campaigns using a exchange rate of ' + str(MONERO_EXCHANGE) +  ' USD. [active]* by November 2018}\\label{tab:aggregated_payments}'
    print '\\end{table}'

     
    return top_campaigns

def print_github_info(filename='../data/aggregated_campaigns_all_second_round.pickle'):
    #--------- MODE: WITHOUT POOLS IN THE ADDRESS
    withpool = False 
    allCampaigns = pickle.load(open(filename)) 

    monero=get_Monero_campaigns()
    campaigns={}
    for c in monero:
        campaigns[c]=allCampaigns[c]
    print "MONERO CAMPAIGNS",len(campaigns)
    
    value_totalpaid, info_totalpaid_dic, info_totalpaid, all_paid, all_totalUSD,all_wallets, all_samples,still_active,totalCampaignsWithData,veryInitialDate  = get_value_totalpaid(campaigns, withpool,include_activity_period=True)

    orderedByXMR=sorted(value_totalpaid, key=lambda k:value_totalpaid[k], reverse=True)
    orderedByWallets=sorted(info_totalpaid_dic, key=lambda k:info_totalpaid_dic[k]['wallets'], reverse=True)
    orderedBySamples=sorted(info_totalpaid_dic, key=lambda k:info_totalpaid_dic[k]['samples'], reverse=True)
    top_campaigns = []    
    topFirstPayment=datetime.now()
    print "# Campaigns"
    print
    print "All 2K campaigns are listed in [here](all_graphs)."
    print
    # BY XMR
    print "## Top 10 campaigns (ranked by XMR) ##"
    print
    print "| Campaign | Total Paid | Num. Samples | Num. Wallets |  First Payment |"
    print "| --- | --- | --- | --- | --- |"
    for key in orderedByXMR[:10]:
        samples = info_totalpaid_dic[key]['samples']
        wallets = info_totalpaid_dic[key]['wallets']
        xmr = info_totalpaid_dic[key]['total_paid'] 
        usd = info_totalpaid_dic[key]['totalUSD'] 
        started=info_totalpaid_dic[key]['firstPayment']
        print '| [%s](all_graphs/C%s_updated.pdf)'%(key,key)+' | **{0:,.0f} XMR** ({1:s} USD) | {2:,.0f} | {3:,.0f} '.format(xmr,millify(usd),samples,wallets)+'|  %s |'%started.strftime("%Y-%m-%d")
        #print '[Campaign %s](all_graphs/C%s_updated.pdf)'%(key,key)+' Total paid: **%.2f XMR** (%.2f USD). Num. samples: %d. Num. Wallets: %d. First payment: %s <br/>'%(key,key,xmr,millify(usd),samples,wallets,started)
    # BY samples
    print
    print "## Top 10 campaigns (ranked by size -- num. samples) ##"
    print "| Campaign | Total Paid | Num. Samples | Num. Wallets |  First Payment |"
    print "| --- | --- | --- | --- | --- |"
    for key in orderedBySamples[:10]:
        samples = info_totalpaid_dic[key]['samples']
        wallets = info_totalpaid_dic[key]['wallets']
        xmr = info_totalpaid_dic[key]['total_paid'] 
        usd = info_totalpaid_dic[key]['totalUSD'] 
        started=info_totalpaid_dic[key]['firstPayment']
        print '| [%s](all_graphs/C%s_updated.pdf)'%(key,key)+' | {0:,.0f} XMR ({1:s} USD) | **{2:,.0f}** | {3:,.0f} '.format(xmr,millify(usd),samples,wallets)+'|  %s |'%started.strftime("%Y-%m-%d")
        # print '[Campaign %s](all_graphs/C%s_updated.pdf). Total paid: %.2f XMR (%.2f USD). **Num. samples: %d**. Num. Wallets: %d. First payment: %s <br/>'%(key,key,xmr,millify(usd),samples,wallets,started)    
    # BY wallets
    print
    print "## Top 10 campaigns (ranked by size -- num. wallets) ##"
    print "| Campaign | Total Paid | Num. Samples | Num. Wallets |  First Payment |"
    print "| --- | --- | --- | --- | --- |"    
    for key in orderedByWallets[:10]:
        samples = info_totalpaid_dic[key]['samples']
        wallets = info_totalpaid_dic[key]['wallets']
        xmr = info_totalpaid_dic[key]['total_paid'] 
        usd = info_totalpaid_dic[key]['totalUSD'] 
        started=info_totalpaid_dic[key]['firstPayment']
        print '| [%s](all_graphs/C%s_updated.pdf)'%(key,key)+' | {0:,.0f} XMR ({1:s} USD) | {2:,.0f} | **{3:,.0f}** '.format(xmr,millify(usd),samples,wallets)+'|  %s |'%started.strftime("%Y-%m-%d")
        # print '[Campaign %s](all_graphs/C%s_updated.pdf). Total paid: **%.2f** XMR (%.2f USD). Num. samples: %d. **Num. Wallets: %d**. First payment: %s <br/>'%(key,key,xmr,millify(usd),samples,wallets,started)
    
    # DISCLAIMER
    # This dict must be updated whenever graphs are created, since campaigns' names are hardcoded.
    # 
    known_campaigns={8:("Photominer","Guardicore","https://perma.cc/JE3Y-F42L","June, 2016"),
                    1290:("Adylkuzz","Proofpoint","https://perma.cc/3V7G-CDEN","May, 2017"),
                    483:("Smominru","Proofpoint","https://perma.cc/V5UR-TDLU","January, 2018"),
                    148:("Jenkins","Checkpoint","https://perma.cc/SVN4-C5B4","February, 2018"),
                    29:("Xbooster","Netskope","https://perma.cc/8RZG-5QBS","May, 2018"),
                    611:("Rocke","CISCO Talos","https://perma.cc/ZH4B-DBG3","August, 2018")
                    }
    print "## Campaigns previously reported ##"
    print "| Name | Campaign | Total Paid | Num. Samples | Num. Wallets |  First Payment | Date reported | Report |"
    print "| --- | --- | --- | --- | --- | --- | --- | --- |"
    for key,(name,author,URL,date) in known_campaigns.items():
        samples = info_totalpaid_dic[key]['samples']
        wallets = info_totalpaid_dic[key]['wallets']
        xmr = info_totalpaid_dic[key]['total_paid'] 
        usd = info_totalpaid_dic[key]['totalUSD'] 
        started=info_totalpaid_dic[key]['firstPayment']        
        print '| %s | [%s](all_graphs/C%s_updated.pdf) | '%(name,key,key)+' {0:,.0f} XMR ({1:s} USD) | {2:,.0f} | {3:,.0f} | '.format(xmr,millify(usd),samples,wallets)+' %s | %s | [%s](%s) |'%(started.strftime("%Y-%m-%d"),date,author,URL)
    print
    # print "## Campaigns previously reported ##"
    # print "[Photominer (Campaign 5)](all_graphs/C5_updated.pdf). [Report by Guardicore](https://perma.cc/JE3Y-F42L). June, 2016<br/>"
    # print "[Adylkuzz (Campaign 1298)](all_graphs/C1298_updated.pdf). [Report by Proofpoint](https://perma.cc/3V7G-CDEN). May, 2017<br/>"
    # print "[Smominru (Campaign 480)](all_graphs/C480_updated.pdf). [Report by Proofpoint](https://perma.cc/V5UR-TDLU). January, 2018<br/>"
    # print "[Jenkins (Campaign 143)](all_graphs/C143_updated.pdf). [Report by Checkpoint](https://perma.cc/SVN4-C5B4). February, 2018<br/>"
    # print "[Xbooster (Campaign 27)](all_graphs/C27_updated.pdf). [Report by Netskope](https://perma.cc/8RZG-5QBS). May, 2018<br/>"
    # print "[Rocke (Campaign 606)](all_graphs/C606_updated.pdf). [Report by CISCO Talos](https://perma.cc/ZH4B-DBG3). August, 2018<br/>"
def print_campaign2latex(filename='../data/aggregated_campaigns_second_round.pickle',onlyMonero=False,include_activity_period=False):

    #--------- MODE: WITHOUT POOLS IN THE ADDRESS
    withpool = False 
    allCampaigns = pickle.load(open(filename)) 
    
    if onlyMonero:
        monero=get_Monero_campaigns()
        campaigns={}
        for c in monero:
            campaigns[c]=allCampaigns[c]
        print "MONERO CAMPAIGNS",len(campaigns)
    else:
        campaigns = allCampaigns
    return campaign2latex(campaigns, withpool,include_activity_period=include_activity_period)


def aggregate_all():
    compressed_parents_map, ssdeep_map, authentihash_map, authentihash, execution_parents_map = aggregate_samples(PATH_REPORTS)
    json.dump(compressed_parents_map, open('../data/mergeWallets_compressed_parents_map.json', 'w+'))
    json.dump(execution_parents_map, open('../data/mergeWallets_execution_parents_map.json', 'w+'))
    json.dump(ssdeep_map, open('../data/mergeWallets_ssdeep_map.json', 'w+'))
    json.dump(authentihash_map, open('../data/mergeWallets_authentihash_map.json', 'w+'))
    json.dump(authentihash, open('../data/mergeWallets_authentihash.json', 'w+'))



def get_samples_in_campaigns(inputfile = '../data/graph_wallets_all.gexf'):
    # ------- Get campaigns
    G = nx.read_gexf(inputfile)
    campaigns = evaluate_connected_subgraphs(G)

    with open('../data/parents-linkedwallets.txt', 'w') as f: 
        for campaign_id in campaigns:
            samples = campaigns[campaign_id]['samples']
            for sample in samples:
                f.write(sample + '\n')


def get_number_samples_campaigns(inputfile):
    G = nx.read_gexf(inputfile)
    campaigns = evaluate_connected_subgraphs(G)
    return fetch_number_samples_campaigns(campaigns)

def fetch_number_samples_campaigns(campaigns):
    num_samples = {}
    num_wallets = {}
    earnings = {}

    monero = get_Monero_campaigns()

    for campaign_id in campaigns:
        samples = campaigns[campaign_id]['samples']
        num_samples[campaign_id] = len(samples)

        wallets = campaigns[campaign_id]['wallets']
        num_wallets[campaign_id] = len(wallets)

        if campaign_id in monero:
            totalPaid,totalSamples,totalWallets,poolData,totalUSD=stats_wallets(wallets,numTab=0,verbose=False)
            if totalPaid != None:
                earnings[campaign_id] = float(totalPaid)

    return num_samples, num_wallets, earnings


def print_numberSamplesCampaigns2vector(aggregated_campaigns_filename = '../data/aggregated_campaigns_second_round.pickle'):
    
    #inputfile = '../data/graph_wallets_all.gexf'
    #print '%//', inputfile
    #num_samples, num_wallets = get_number_samples_campaigns(inputfile)

    campaigns = pickle.load(open(aggregated_campaigns_filename)) 
    num_samples, num_wallets, earnings = fetch_number_samples_campaigns(campaigns)

    print 'numberSamplesCampaignsWithoutpool = ', str(sorted(num_samples.values())).replace(',', ' ') + ';'
    print 'numberWalletsCampaignsWithoutpool = ', str(sorted(num_wallets.values())).replace(',', ' ') + ';'
    print 'earnings = ', str(sorted(earnings.values())).replace(',', ' ') + ';'
        


def get_first_seen(shash):
    path = os.path.join(PATH_REPORTS, shash + '.report')
    json_report = pickle.load(open(path, "rb" ))
    #if 'additional_info' in json_report and 'first_seen_itw' in json_report['additional_info']:
    #    return json_report['additional_info']['first_seen_itw']

    if 'first_seen' in json_report: 
        return json_report['first_seen']

    return None


def build_quarter_string(year, month):
    numeric_quarter = int(math.ceil(month/3.0))
    return str(year) + 'Q' + str (numeric_quarter)
def build_semester_string(year, month):
    numeric_semester = int(math.ceil(month/6.0))
    return str(year) + 'S' + str (numeric_semester)

def get_vt_period(vt_date,minYear=2014):
    if '.' in vt_date and 'T' in vt_date and 'Z' in vt_date:
        date_object = datetime.strptime(vt_date, '%Y-%m-%dT%H:%M:%S.%fZ')
    else:
        date_object = datetime.strptime(vt_date, '%Y-%m-%d %H:%M:%S')
    if date_object.year<minYear:
        return "BEFORE-"+str(minYear),"BEFORE-"+str(minYear),"BEFORE-"+str(minYear)
    quarter_year = build_quarter_string(date_object.year, date_object.month)
    month_year = str(date_object.year)+'M'+str(date_object.month)
    semester_year = build_semester_string(date_object.year, date_object.month)
    return month_year,quarter_year,semester_year


all_quarters_ordered = ['2009Q1', '2009Q2', '2009Q3', '2009Q4', 
                        '2010Q1', '2010Q2', '2010Q3', '2010Q4',
                        '2011Q1', '2011Q2', '2011Q3', '2011Q4',
                        '2012Q1', '2012Q2', '2012Q3', '2012Q4',
                        '2013Q1', '2013Q2', '2013Q3', '2013Q4',
                        '2014Q1', '2014Q2', '2014Q3', '2014Q4',
                        '2015Q1', '2015Q2', '2015Q3', '2015Q4',
                        '2016Q1', '2016Q2', '2016Q3', '2016Q4',
                        '2017Q1', '2017Q2', '2017Q3', '2017Q4',
                        '2018Q1', '2018Q2', '2018Q3']

def analyseEvolutionCampaign(numCampaign):
    print "Creating evolving graphs of campaign %s"%numCampaign
    if os.path.exists('../data/graph_wallets_all_%s.gexf'%numCampaign):
        print "Reading graph from file"
        graph = nx.read_gexf('../data/graph_wallets_all_%s.gexf'%numCampaign)
    else:
        print "Computing graph"
        graph=plot_a_campaign(numCampaign)
    time_data={
    '2016_1':datetime(2016,01,01),
    '2016_2':datetime(2016,06,01),
    '2017_1':datetime(2017,01,01),
    '2017_2':datetime(2017,06,01),
    '2018_1':datetime(2018,01,01),
    '2018_2':datetime(2018,06,01)
    }
    # time_data={
    # '2017_08':datetime(2017,8,1),
    # '2017_12':datetime(2017,12,1),
    # '2018_1':datetime(2018,1,1),
    # '2018_5':datetime(2018,5,1),
    # '2018_6':datetime(2018,6,1)
    # }
    toRemove=[]
    # timestamps=[]
    # for edge in graph.nodes(data=True):
    #     sample = edge[0]
    #     if sample in dataset:
    #         timestamps.append(datetime.strptime(dataset[sample]['fs'],'%Y-%m-%d %H:%M:%S'))
    # for ts in sorted(timestamps):
    #     print ts
    # exit()
    
    for name,limit in time_data.items():
        print '\tPruning at %s'%limit
        graph_copy=copy.deepcopy(graph)
        for edge in graph_copy.nodes(data=True):
            sample = edge[0]
            # Remove sample if it was first seen after this period
            if sample in dataset and dataset[sample]['fs'] is not None and datetime.strptime(dataset[sample]['fs'],'%Y-%m-%d %H:%M:%S')>limit:
                graph_copy.remove_node(sample)
        outfile='../data/graph_campaign_%s_%s.gexf'%(numCampaign,name)
        nx.write_gexf(graph_copy, outfile) 
        print "\tGraph of %s has %s nodes and %s edges"%(name,len(graph_copy),graph_copy.number_of_edges())
        print '\tDumped at', outfile


def analyseCampaign(numCampaign,inputfile = '../data/graph_wallets_all.gexf',latex=False):
    if not os.path.exists('../data/aggregated_campaigns_all.pickle'):
        print "'../data/aggregated_campaigns_all.pickle' NOT FOUND"
        return
    else:
        if not latex: print "READING CAMPAIGNS FROM FILE"
        campaigns=pickle.load(open('../data/aggregated_campaigns_all.pickle'))
    if not latex: print "GETTING WALLETS OF CAMPAIGN %s"%numCampaign
    wallets_campaign=campaigns[numCampaign]['wallets']
    if not latex: print "ANALYSING WALLETS"
    if latex:
        analyse_campaign_by_wallets("C#"+str(numCampaign),wallets_campaign,latex=True,show_per_wallet_info=False,include_hash_rate=False)
    else:
        analyse_campaign_by_wallets("C#"+str(numCampaign),wallets_campaign,latex=False,show_per_wallet_info=True,include_hash_rate=True)
  

def printFamilyViolin(hash_by_campaign, campaign_map, hash_by_quarter, white_list = None, filename = ''):

    import numpy as np
    import matplotlib.pyplot as plt   
    plt.ioff() 
    import datetime

    data = [] 

    campaign_keys_ordered_by_first_seen_quarter = []
    min_quarter = '9999Q9'
    max_quarter = '0000Q0'

    quarters_ordered_with_samples = []
    for quarter in all_quarters_ordered:
        if not quarter in aze_by_quarter:
            continue
        for sha256 in hash_by_quarter[quarter]:
            if not sha256 in campaign_map:
                continue
            campaignId = campaign_map[sha256]
            if white_list and campaignId not in white_list: 
                continue
            if campaignId not in campaign_keys_ordered_by_first_seen_quarter:
                campaign_keys_ordered_by_first_seen_quarter.append(campaignId)
            if quarter not in quarters_ordered_with_samples:
                quarters_ordered_with_samples.append(quarter)
            if quarter < min_quarter:
                min_quarter = quarter
            if quarter > max_quarter:
                max_quarter = quarter

    if min_quarter > all_quarters_ordered[-1] and max_quarter < all_quarters_ordered[0]:
        quarters_range = []
    else: 
        quarters_range = all_quarters_ordered[all_quarters_ordered.index(min_quarter):all_quarters_ordered.index(max_quarter)+1]

    print 'quarters_with_samples = ', quarters_ordered_with_samples
    print 'quarters = ', quarters_range

    campaigns = []
    unseen_hasesh = 0

    for campaignId in campaign_keys_ordered_by_first_seen_quarter:

        hit_quarters = [] # quarters_in_which_a_sample_of_a_family_was_seen

        if white_list and not campaignId in white_list:
            continue

        campaigns.append(campaignId)

        for chash in hash_by_campaign[campaignId]:

            if chash in aze_by_sha256:
                aze = aze_by_sha256[chash]
                quarter_year = get_quarter_year(aze)
                hit_quarters.append(quarter_year)
            else:
                unseen_hasesh += 1


        #values = [samples_per_quarter[key] for key in sorted(samples_per_quarter.iterkeys())]

        values = []
        for hit in hit_quarters: 
            values.append(quarters_range.index(hit))

        data.append(np.array(values))

    print 'WARNING {} hashes not in Latest'.format(unseen_hasesh)

    hash_by_campaign = None
    campaign_map = None 
    hash_by_quarter = None

    print 'campaigns = ', campaigns

    fig, axes = plt.subplots()
    axes.violinplot(data, range(len(data)), points=80, vert=False, widths=0.7,
                          showmeans=True, showextrema=True, showmedians=True)

    axes.set_yticks(np.arange(0, len(campaigns)))
    axes.set_yticklabels(campaigns, fontsize = 14.0)
    
    #indexes = np.arange(0, len(quarters_range) + 1, 2)
    #indexes[-1] = len(quarters_range) - 1
    ##if len(quarters_range) > indexes[-1]:
    ##    indexes = np.append(indexes, len(quarters_range)-1)
    #quarters_plot = [quarters_range[i] for i in indexes if i < len(quarters_range)]
    #for i in range(len(quarters_plot)):
    #    if 'Q1' in quarters_plot[i]:
    #        #quarters_plot[i] = quarters_plot[i][:-2]
    #        pass
    #    else: 
    #        quarters_plot[i] = quarters_plot[i][4:]

    indexes = []
    quarters_plot = []
    for i in range(len(quarters_range)):
        indexes.append(i)
        if 'Q1' in quarters_range[i]:
            quarters_plot.append(quarters_range[i][:-2])
        else:
            quarters_plot.append('')

    axes.set_xticks(indexes)
    axes.set_xticklabels(quarters_plot, fontsize = 18.0) #, rotation=225
    plt.subplots_adjust(left=0.21, bottom=0.08, right=0.98, top=0.99) # wspace=0, hspace=0

    fig.savefig(filename + '-' + str(datetime.datetime.now()) + '.pdf')
    #plt.show()
    plt.close(fig)


'''
   Aggregate all: Domains + compressed parents + linkedwallets (samples with the same wallets)
'''
def generate_graph_all(outfile,remove_botnets=False,add_compressed_parents=False):

    #--------- Compressed parents and known childs (generated with aggregate_all)
    compressed_parents = json.load(open('../data/mergeWallets_compressed_parents_map.json'))
    execution_parents = json.load(open('../data/mergeWallets_execution_parents_map.json'))
    childs = pickle.load(open('../data/childs_of_samples.pickle'))

    #--------- Aggregated domains (generated with domainAnalyser.py)
    path = '../data/campaigns_based_on_domains.pickle'
    campaigns_domains = pickle.load(open(path, "rb" ))

    #--------- Aggregated proxies (generated with walletAnalyzer.py)
    path = '../data/campaigns_based_on_proxies.pickle'
    campaigns_proxies = pickle.load(open(path, "rb" ))


    #--------- Aggregated ITW_Urls (generated with data_digger.py)
    path = '../data/campaigns_based_on_itwurls.pickle'
    campaigns_itwurls = pickle.load(open(path, "rb" ))


    #--------- Known Mining Tools
    # mining_tools = pickle.load(open('../data/miner-tools.pickle','rb'))
    # Mining tools through ssdeep
    lines=open('../data/mergeWallets_ssdeep_know_miners.csv')
    for l in lines[1:]:
        sha256=l.split(',')[0]
        tool=l.split(',')[1]
        if not sha256 in mining_tools:
            mining_tools[sha256]={'tools':tool}

    campaigns = {}
    campaigns.update(campaigns_domains)
    campaigns.update(campaigns_proxies)
    campaigns.update(campaigns_itwurls)
    #print "WARNING: I'm ignoring campaigns_itwurls"

    #--------- MODE: WITHOUT POOLS IN THE ADDRESS AND DUPLICATING CHILDS
    withpool = False 
    wallets = get_wallets_per_samples(withpool=withpool)
    G = plot_campaigns(wallets, compressed_parents, execution_parents, childs, campaigns, mining_tools, add_compressed_parents=add_compressed_parents, linkedwallets=True, withpool=withpool, duplicate_childs=True,remove_botnets=remove_botnets) 


    
    print '----- Dumping', outfile
    print len(G.edges())
    print len(G.nodes())
    nx.write_gexf(G, outfile)      


def plot_a_campaign(campaign_id, inputfile = '../data/graph_wallets_all.gexf'):
    
    G = nx.read_gexf(inputfile)
    connected_components = list(nx.connected_component_subgraphs(G))
    outfile = inputfile.replace('.gexf', '_' + str(campaign_id) + '.gexf')

    for i, graph in enumerate(connected_components): 
        if campaign_id == i:
            nx.write_gexf(graph, outfile) 
            print 'Dumped at', outfile
            return graph
            

def split_campaigns(inputfile = '../data/graph_wallets_all.gexf'):

    G = nx.read_gexf(inputfile)
    connected_components = list(nx.connected_component_subgraphs(G))
    dirname = inputfile.replace('.gexf', '')
    if os.path.exists(dirname):
        print 'Deleting contents in', dirname
        shutil.rmtree(dirname)
    os.mkdir(dirname)

    for i, graph in enumerate(connected_components): 
        filename = 'C' + str(i) + '.gexf'
        outfile = os.path.join(dirname, filename)
        nx.write_gexf(graph, outfile) 
        print 'Dumped at', outfile

def addSample(G,sample,mining_tools):
    if sample in dataset and dataset[sample]['fs'] is not None:
        vt_date=dataset[sample]['fs']
        month,quarter,semester=get_vt_period(vt_date)
    else:
        month='UNKNOWN'
        quarter='UNKNOWN'
        semester='UNKNOWN'  

    color = '#F0A825'
    nodetype = 'second-iteration'
    tool = 'no-tool'
    if sample in mining_tools:
        color = '#FDC052'
        tool = mining_tools[sample]['tool']
        nodetype = 'second-iteration_mining_tool'
        sample = sample + '-mining_tools:' + mining_tools[sample]['tool']
    positives = -1
    if sample in dataset and 'positives' in dataset[sample]:
        positives = dataset[sample]['positives']

    if tool=='no-tool':
        tool=''

    G.add_node(sample, label=tool,monthTS=month,quarterTS=quarter,semesterTS=semester, wallet='', positives=positives, color=color, fillcolor=color, type=nodetype, style='filled')
    
    return sample

def included_sample(sample,samples_campaign):
    for s in samples_campaign:
        if sample in s:
            return True
    return False

def update_campaigns(directory_base_name,aggregated_file_name='../data/aggregated_campaigns_all.pickle', add_botnets=True): 

    childs = pickle.load(open('../data/childs_of_samples.pickle'))

    directory_name = directory_base_name.replace('.gexf', '')

    if not os.path.exists(aggregated_file_name):
        print "ERROR: %s does not exists"%aggregated_file_name
        return
    if not os.path.exists(directory_name):
        print "ERROR: %s does not exists"%directory_name
        return
    
    output_directory=directory_name+'_second_round'
    if os.path.exists(output_directory):
        print 'Deleting contents in', output_directory
        shutil.rmtree(output_directory)    
    if not os.path.exists(output_directory):
        os.mkdir(output_directory)        
    campaigns=pickle.load(open(aggregated_file_name))
    osint_campaigns = {}
    #--------- Aggregated ITW_Urls
    path = '../data/campaigns_based_on_itwurls.pickle'
    campaigns_itwurls = pickle.load(open(path, "rb" ))
    osint_campaigns.update(campaigns_itwurls)    

    if add_botnets:
        #--------- Aggregated domains
        path = '../data/campaigns_based_on_domains.pickle'
        campaigns_domains = pickle.load(open(path, "rb" ))
        osint_campaigns.update(campaigns_domains)
    
    mining_tools = pickle.load(open('../data/miner-tools.pickle','rb'))
    # Mining tools through ssdeep
    # lines=open('../data/mergeWallets_ssdeep_know_miners.csv').readlines()
    # for l in lines[1:]:
    #     sha256=l.split(',')[0].strip()
    #     tool=l.split(',')[1].strip()
    #     if not sha256 in mining_tools:
    #         mining_tools[sha256]={'tool':tool}

    
    for campaign in campaigns:
        edge_ids=[]
        try:
            G = nx.read_gexf(directory_name+'/C%s.gexf'%campaign)
            for (s,d) in G.edges():
                edge_ids.append(G.get_edge_data(s,d)['id'])
        except:
            print "Cannot open file %s"%(directory_name+'/C%s.gexf'%campaign)
            continue

        samples_campaign=[]
        for node in G.nodes(data=True):
            #print node
            label = node[0]
            # --- We do not account for artificial nodes added to link wallets or campaigns
            if 'sub-' in label:
                label="-".join(label.split('-')[1:])
            samples_campaign.append(label)
        # Add samples from the subcampaigns that were missing in the first round.

        seen = []
        for s in samples_campaign:
            seen.append(s)
        for subcampaign in osint_campaigns:
            add=False
            samples_subcampaign=osint_campaigns[subcampaign]['samples']
            # If sample is in ITWURL subcampaign but it was not included in the campaign:
            # if subcampaign.startswith('ITWURL') and subcampaign in samples_campaign:
            if subcampaign in samples_campaign:
                newSamples = [sample for sample in samples_subcampaign if not included_sample(sample,samples_campaign)]
                # Add new samples
                for sample in newSamples:
                    if is_not_malware(sample):
                        continue
                    if sample not in seen: 
                        hash_sample=sample
                        sample=addSample(G,sample,mining_tools)
                        print "Campaign %s: new Node %s"%(campaign,sample)
                        seen.append(hash_sample)
                    print "Campaign %s: edge from %s to %s"%(campaign,sample,subcampaign)

                    newid=randint(1,100000)
                    while str(newid) in edge_ids:
                        newid=randint(1,100000)
                    edge_ids.append(newid)
                    G.add_edge(sample, 'sub-'+subcampaign, id=str(newid),weight=1, color='black',typeEdge='same_subcampaign') 

            # If sample is in one of the BOTNETS and it was part of this campaign
            elif add_botnets:
                toAdd=["RAMNIT","VIRUT","NITOL-BOTNET"]
                for botnet in toAdd:
                    if botnet in subcampaign:
                        add=True
                        break
                if add:
                    samples_to_botnet=[sample for sample in samples_subcampaign if included_sample(sample,samples_campaign)]
                    if len(samples_to_botnet)>0:
                        name="OSINT_"+"-".join(subcampaign.split('-')[1:])
                        label="-".join(subcampaign.split('-')[1:])
                        color = '#5291FD'
                        nodetype='domain_subcampaign_osint'
                        print "Campaign %s: Detected %s samples (%.2f %%) using botnet %s"%(campaign,len(samples_to_botnet),(len(samples_to_botnet)*100.0/len(samples_campaign)),botnet)

                        subcampaign_name="'sub-%s'"%subcampaign.replace(':','_')
                        G.add_node(subcampaign_name,label=label, wallet='', campaign='sub-'+name, color=color, fillcolor=color, type=nodetype, style='filled', fontsize=FONTSIZE)
                        for sample in samples_to_botnet:
                            print "Campaign %s: edge from %s to %s"%(campaign,sample,subcampaign)
                            newid=randint(1,100000)
                            while str(newid) in edge_ids:
                                newid=randint(1,100000)
                            edge_ids.append(newid)
                            G.add_edge(sample, subcampaign_name,id=str(newid), weight=1, color='black' , typeEdge='same_subcampaign') 
        
        for sample in samples_campaign:
            if sample not in childs:
                continue
            for idx, hash_child in enumerate(childs[sample]):
                
                child = hash_child

                if child in dataset and dataset[child]['fs'] is not None:
                    vt_date=dataset[child]['fs']
                    month,quarter,semester=get_vt_period(vt_date)
                else:
                    month='UNKNOWN'
                    quarter='UNKNOWN'
                    semester='UNKNOWN'  

                # Add details about the mining tool
                color = '#F0A825'
                nodetype = 'child_node'
                tool='no-tool'
                fontsize='14'
                if hash_child in mining_tools:
                    color = '#FDC052'
                    nodetype = 'child_node_mining_tool'
                    child = child + '-mining_tools:' + mining_tools[hash_child]['tool']
                    tool=mining_tools[hash_child]['tool']
                    fontsize=FONTSIZE

                if child not in seen:

                    positives = -1
                    if hash_child in dataset and 'positives' in dataset[hash_child]:
                        positives = dataset[hash_child]['positives']

                    G.add_node(child,label='', monthTS=month,quarterTS=quarter,semesterTS=semester, wallet='', positives=positives, color=color, fillcolor=color, type=nodetype, style='filled',tool=tool, fontsize=fontsize)
                    seen.append(child)
                newid=randint(1,100000) 
                while str(newid) in edge_ids:
                    newid=randint(1,100000)
                edge_ids.append(newid)
                G.add_edge(sample, child, id=str(newid),weight=1, color='black', typeEdge='same_family')


        filename = 'C%s_updated.gexf'%campaign
        outfile = os.path.join(output_directory, filename)
        nx.write_gexf(G, outfile) 
        #print 'Dumped at', outfile

def analyse_currency_campaigns(filename='../data/currency_campaigns.pickle',aggregated_file_name='../data/aggregated_campaigns_all.pickle'):

    campaigns=pickle.load(open(aggregated_file_name))
    print "TOTAL CAMPAIGNS:",len(campaigns)
    currency_campaigns={}
    for c in campaigns:
        currencies=[]
        wallets=campaigns[c]['wallets']
        if len(wallets)==0:
            currency_campaigns[c]=['NO-WALLETS']    
            continue
        for w in wallets:
            currency=get_currency(w)
            if not currency in currencies:
                currencies.append(currency)
        currency_campaigns[c]=currencies
    pickle.dump(currency_campaigns,open(filename,'wb'))

def get_Monero_campaigns(filename='../data/currency_campaigns.pickle'):
    currency_campaigns=pickle.load(open(filename))
    monero_campaigns=[]
    for c in currency_campaigns:
        if 'MONERO' in currency_campaigns[c]:
            monero_campaigns.append(c)
    return monero_campaigns

def stats_per_currency(only=False,filename='../data/currency_campaigns.pickle'):
    currency_campaigns=pickle.load(open(filename))
    campaigns_currency={'EMAIL':0,'ELECTRONEUM':0,'MONERO':0,'BITCOIN':0,'ZCASH':0,'ETHEREUM':0,'TURTLECOIN':0,'AEON':0,'BYTECOIN':0,'INTENSECOIN':0,'SUMOKOIN':0,'CRYPTONITE':0,'OTHER':0,'NO-WALLETS':0}
    mix_campaigns=0
    for campaign in currency_campaigns:
        if len(currency_campaigns[campaign])>1:
            different_currencies=0
            for currency in currency_campaigns[campaign]:
                if not only:
                    campaigns_currency[currency]+=1
                if currency=='NotWallet':
                    print "NOT A WALLET"
                if not currency=='EMAIL' and not currency=='OTHER' and not currency=='NO-WALLETS':
                    different_currencies+=1
            if different_currencies>=2:
                mix_campaigns+=1
        elif len(currency_campaigns[campaign])==1:
            currency=currency_campaigns[campaign][0]
            campaigns_currency[currency]+=1
        else:
            print'WARNING: campaign %s has no currencies'%campaign
        

    for currency in sorted(campaigns_currency,key=lambda x:campaigns_currency[x],reverse=True):
        print "%s & %d"%(currency,campaigns_currency[currency])
        #print campaigns_currency[currency][:10]
    print "MIXED & %d"%mix_campaigns
    print "TOTAL & %d"%(len(currency_campaigns))



if __name__ == "__main__" :            

    
    # ----- Look for childs
    search4childs.look_for_childs()
    # search4childs.look_for_brothers()


    outfile = '../data/graph_wallets_all.gexf'    
    aggregated_file_name='../data/aggregated_campaigns_all.pickle' 

    # ----- Aggregate all
    aggregate_all()
    generate_graph_all(outfile,remove_botnets=True,add_compressed_parents=False)

    # # ----- Extract graphs for given sub-campaings
    split_campaigns_and_evaluate_connected_subgraphs(inputfile=outfile,aggregated_file_name=aggregated_file_name)
    update_campaigns(outfile,aggregated_file_name=aggregated_file_name,add_botnets=True)
     
    directory_name = outfile.replace('.gexf', '')
    output_directory=directory_name+'_second_round'
    evaluate_subgraphs_from_directory(directory_name=output_directory,writeToFile=True)
    
    aggregated_file_name='../data/aggregated_campaigns_second_round.pickle'
    plot_subgraphs_from_directory(directory_name=output_directory,aggregated_file_name=aggregated_file_name)
    

    analyse_currency_campaigns(filename='../data/currency_campaigns.pickle')
    stats_per_currency(filename='../data/currency_campaigns.pickle')

    
    top_campaigns=print_campaign2latex(filename='../data/aggregated_campaigns_second_round.pickle' ,onlyMonero=True,include_activity_period=True)
    
    # Github README info generation
    for (numCampaign,value) in top_campaigns:
        print '[Campaign %s](all_graphs/C%s_updated.pdf). Total paid: %.2f XMR <br/>'%(numCampaign,numCampaign,value)

    print_github_info(filename=aggregated_file_name)

    print 'Done'
